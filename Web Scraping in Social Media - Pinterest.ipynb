{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0901af7",
   "metadata": {},
   "source": [
    "## Web Scraping in Social Media - Pinterest\n",
    "\n",
    "### The project involves leveraging web scraping techniques to gather valuable insights from social media platforms like Pinterest, focusing on extracting image data and pin details for analysis and application in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114bccf",
   "metadata": {},
   "source": [
    "## Pinterest Pin Data Retrieval Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de468e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Pinterest username: MariMoon\n",
      "Saved pin details to MariMoon_pins.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_pinterest_pins(username, max_pins=10):\n",
    "    pin_details = []\n",
    "    \n",
    "    # Setup the Edge WebDriver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless') \n",
    "    options.add_argument('--disable-gpu')\n",
    "    \n",
    "    service = EdgeService(EdgeChromiumDriverManager().install())\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://www.pinterest.com/{username}/\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(15)  # Increase waiting time for initial page load\n",
    "        \n",
    "        # Scroll down to load more pins (if needed)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(15)  # Increase waiting time after scrolling\n",
    "        \n",
    "        pins = driver.find_elements(By.CSS_SELECTOR, 'img[src][alt]')\n",
    "        for pin in pins[:max_pins]:\n",
    "            pin_data = {}\n",
    "            \n",
    "            # Extracting pin details\n",
    "            src = pin.get_attribute('src')\n",
    "            alt = pin.get_attribute('alt')\n",
    "            \n",
    "            # Storing pin details in a dictionary\n",
    "            pin_data['Image URL'] = src\n",
    "            pin_data['Alt Text'] = alt\n",
    "            \n",
    "            pin_details.append(pin_data)\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "     # Saving pin details to a CSV file\n",
    "        df = pd.DataFrame(pin_details)\n",
    "        df.to_csv(f\"{username}_pins.csv\", index=False)\n",
    "        print(f\"Saved pin details to {username}_pins.csv\")\n",
    "\n",
    "# Main block to take user input\n",
    "if __name__ == \"__main__\":\n",
    "    username = input(\"Enter the Pinterest username: \")\n",
    "    fetch_pinterest_pins(username, max_pins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a68df",
   "metadata": {},
   "source": [
    "## Pinterest Image Scraping Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eed8b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the keyword for searching images: nature\n",
      "Downloading completed !!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pydotmap import DotMap\n",
    "\n",
    "class PinterestImageScraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.json_data_list = []  # List to store JSON data from Pinterest\n",
    "        self.unique_img = []  # List to store unique image hashes\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pinterest_links(body, max_images):\n",
    "        searched_urls = []\n",
    "        html = soup(body, 'html.parser')  # Parse the HTML content\n",
    "        links = html.select('#b_results cite')  # Select all links within 'cite' tags under '#b_results'\n",
    "        for link in links:\n",
    "            link = link.text\n",
    "            if \"pinterest\" in link:\n",
    "                searched_urls.append(link)\n",
    "                if max_images and len(searched_urls) >= max_images:  # Stop if we have enough URLs\n",
    "                    break\n",
    "        return searched_urls\n",
    "\n",
    "    def get_source(self, url, proxies):\n",
    "        try:\n",
    "            res = get(url, proxies=proxies)  # Get the page content\n",
    "            html = soup(res.text, 'html.parser')\n",
    "            json_data = html.find(\"script\", {\"id\": \"__PWS_INITIAL_PROPS__\"}) or html.find(\"script\", {\"id\": \"__PWS_DATA__\"})\n",
    "            if json_data:\n",
    "                self.json_data_list.append(json.loads(json_data.string)) # Append JSON data to the list\n",
    "            else:\n",
    "                self.json_data_list.append({})\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def save_image_url(self, max_images):\n",
    "        url_list = []\n",
    "        for js in self.json_data_list:\n",
    "            try:\n",
    "                data = DotMap(js)  # Convert dictionary to DotMap for easier access\n",
    "                if data.initialReduxState:\n",
    "                    pins = data.initialReduxState.pins\n",
    "                else:\n",
    "                    pins = data.props.initialReduxState.pins\n",
    "                for pin in pins.values():\n",
    "                    images = pin.images.get(\"orig\")\n",
    "                    if isinstance(images, list):\n",
    "                        for img in images:\n",
    "                            url_list.append(img.get(\"url\"))\n",
    "                    else:\n",
    "                        url_list.append(images.get(\"url\"))\n",
    "                if max_images and len(url_list) >= max_images:  # Stop if we have enough URLs\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        return list(set(url_list))[:max_images]  # Return unique URLs limited by max_images\n",
    "\n",
    "    def dhash(self, image, hash_size=8):\n",
    "        resized = cv2.resize(image, (hash_size + 1, hash_size))  # Resize the image\n",
    "        diff = resized[:, 1:] > resized[:, :-1]  # Compute the difference\n",
    "        return sum([2 ** i for i, v in enumerate(diff.flatten()) if v])  # Return hash\n",
    "\n",
    "    def save_images(self, url_list, folder_name):\n",
    "        if not os.path.exists(folder_name):  # Create directory if it doesn't exist\n",
    "            os.makedirs(folder_name)\n",
    "        for img_url in url_list:\n",
    "            result = get(img_url, stream=True).content  # Get image content\n",
    "            img_arr = np.asarray(bytearray(result), dtype=\"uint8\")  # Convert to NumPy array\n",
    "            image = cv2.imdecode(img_arr, cv2.IMREAD_COLOR)  # Decode the image\n",
    "            img_hash = self.dhash(image)\n",
    "            if img_hash not in self.unique_img:  # Check if image is unique\n",
    "                file_name = os.path.join(folder_name, img_url.split(\"/\")[-1])\n",
    "                cv2.imwrite(file_name, image)  # Save the image\n",
    "                self.unique_img.append(img_hash)   # Add hash to unique_img\n",
    " \n",
    "    def download(self, url_list, num_workers, output_folder):\n",
    "        url_list = url_list[:10]  # Limit to 10 images\n",
    "        idx = len(url_list) // num_workers if len(url_list) > num_workers else len(url_list)\n",
    "        param = [(url_list[i * idx: (i + 1) * idx], output_folder) for i in range(num_workers)]\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            executor.map(lambda p: self.save_images(*p), param)  # Use ThreadPoolExecutor for concurrent downloads\n",
    "\n",
    "    @staticmethod\n",
    "    def start_scraping(max_images, key, proxies):\n",
    "        assert key, \"Please provide a keyword for searching images\"\n",
    "        keyword = key.replace(\" \", \"%20\") + \"%20pinterest\"\n",
    "        url = f'https://www.bing.com/search?q={keyword}&first=1&FORM=PERE'\n",
    "        res = get(url, proxies=proxies, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        return PinterestImageScraper.get_pinterest_links(res.content, max_images), res.status_code\n",
    "\n",
    "    def scrape(self, key, output_folder=\"\", proxies=None, threads=10, max_images=10):\n",
    "        if proxies is None:\n",
    "            proxies = {}\n",
    "        extracted_urls, search_engine_status_code = PinterestImageScraper.start_scraping(max_images, key, proxies)\n",
    "        self.unique_img = []\n",
    "        self.json_data_list = []\n",
    "\n",
    "        for url in extracted_urls:\n",
    "            self.get_source(url, proxies)\n",
    "\n",
    "        urls_list = self.save_image_url(max_images)\n",
    "\n",
    "        if urls_list:\n",
    "            self.download(urls_list, threads, output_folder or key.replace(\" \", \"_\"))\n",
    "            return {\"isDownloaded\": True, \"urls_list\": urls_list, \"keyword\": key}\n",
    "        return {\"isDownloaded\": False, \"urls_list\": [], \"keyword\": key}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keyword = input(\"Enter the keyword for searching images: \")\n",
    "    output_folder = keyword.replace(\" \", \"_\")\n",
    "    scraper = PinterestImageScraper()\n",
    "    details = scraper.scrape(keyword, output_folder, {}, 10, 10)\n",
    "    if details[\"isDownloaded\"]:\n",
    "        print(\"Downloading completed !!\")\n",
    "    else:\n",
    "        print(\"Nothing to download !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c4a6cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the keyword for searching images: ocean\n",
      "Downloading completed !!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pydotmap import DotMap\n",
    "\n",
    "class PinterestImageScraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.json_data_list = []  # List to store JSON data from Pinterest\n",
    "        self.unique_img = []  # List to store unique image hashes\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pinterest_links(body, max_images):\n",
    "        searched_urls = []\n",
    "        html = soup(body, 'html.parser')  # Parse the HTML content\n",
    "        links = html.select('#b_results cite')  # Select all links within 'cite' tags under '#b_results'\n",
    "        for link in links:\n",
    "            link = link.text\n",
    "            if \"pinterest\" in link:\n",
    "                searched_urls.append(link)\n",
    "                if max_images and len(searched_urls) >= max_images:  # Stop if we have enough URLs\n",
    "                    break\n",
    "        return searched_urls\n",
    "\n",
    "    def get_source(self, url, proxies):\n",
    "        try:\n",
    "            res = get(url, proxies=proxies)  # Get the page content\n",
    "            html = soup(res.text, 'html.parser')\n",
    "            json_data = html.find(\"script\", {\"id\": \"__PWS_INITIAL_PROPS__\"}) or html.find(\"script\", {\"id\": \"__PWS_DATA__\"})\n",
    "            if json_data:\n",
    "                self.json_data_list.append(json.loads(json_data.string)) # Append JSON data to the list\n",
    "            else:\n",
    "                self.json_data_list.append({})\n",
    "        except Exception:\n",
    "            return\n",
    "\n",
    "    def save_image_url(self, max_images):\n",
    "        url_list = []\n",
    "        for js in self.json_data_list:\n",
    "            try:\n",
    "                data = DotMap(js)  # Convert dictionary to DotMap for easier access\n",
    "                if data.initialReduxState:\n",
    "                    pins = data.initialReduxState.pins\n",
    "                else:\n",
    "                    pins = data.props.initialReduxState.pins\n",
    "                for pin in pins.values():\n",
    "                    images = pin.images.get(\"orig\")\n",
    "                    if isinstance(images, list):\n",
    "                        for img in images:\n",
    "                            url_list.append(img.get(\"url\"))\n",
    "                    else:\n",
    "                        url_list.append(images.get(\"url\"))\n",
    "                if max_images and len(url_list) >= max_images:  # Stop if we have enough URLs\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "        return list(set(url_list))[:max_images]  # Return unique URLs limited by max_images\n",
    "\n",
    "    def dhash(self, image, hash_size=8):\n",
    "        resized = cv2.resize(image, (hash_size + 1, hash_size))  # Resize the image\n",
    "        diff = resized[:, 1:] > resized[:, :-1]  # Compute the difference\n",
    "        return sum([2 ** i for i, v in enumerate(diff.flatten()) if v])  # Return hash\n",
    "\n",
    "    def save_images(self, url_list, folder_name):\n",
    "        if not os.path.exists(folder_name):  # Create directory if it doesn't exist\n",
    "            os.makedirs(folder_name)\n",
    "        for img_url in url_list:\n",
    "            result = get(img_url, stream=True).content  # Get image content\n",
    "            img_arr = np.asarray(bytearray(result), dtype=\"uint8\")  # Convert to NumPy array\n",
    "            image = cv2.imdecode(img_arr, cv2.IMREAD_COLOR)  # Decode the image\n",
    "            img_hash = self.dhash(image)\n",
    "            if img_hash not in self.unique_img:  # Check if image is unique\n",
    "                file_name = os.path.join(folder_name, img_url.split(\"/\")[-1])\n",
    "                cv2.imwrite(file_name, image)  # Save the image\n",
    "                self.unique_img.append(img_hash)   # Add hash to unique_img\n",
    " \n",
    "    def download(self, url_list, num_workers, output_folder):\n",
    "        url_list = url_list[:10]  # Limit to 10 images\n",
    "        idx = len(url_list) // num_workers if len(url_list) > num_workers else len(url_list)\n",
    "        param = [(url_list[i * idx: (i + 1) * idx], output_folder) for i in range(num_workers)]\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            executor.map(lambda p: self.save_images(*p), param)  # Use ThreadPoolExecutor for concurrent downloads\n",
    "\n",
    "    @staticmethod\n",
    "    def start_scraping(max_images, key, proxies):\n",
    "        assert key, \"Please provide a keyword for searching images\"\n",
    "        keyword = key.replace(\" \", \"%20\") + \"%20pinterest\"\n",
    "        url = f'https://www.bing.com/search?q={keyword}&first=1&FORM=PERE'\n",
    "        res = get(url, proxies=proxies, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        return PinterestImageScraper.get_pinterest_links(res.content, max_images), res.status_code\n",
    "\n",
    "    def scrape(self, key, output_folder=\"\", proxies=None, threads=10, max_images=10):\n",
    "        if proxies is None:\n",
    "            proxies = {}\n",
    "        extracted_urls, search_engine_status_code = PinterestImageScraper.start_scraping(max_images, key, proxies)\n",
    "        self.unique_img = []\n",
    "        self.json_data_list = []\n",
    "\n",
    "        for url in extracted_urls:\n",
    "            self.get_source(url, proxies)\n",
    "\n",
    "        urls_list = self.save_image_url(max_images)\n",
    "\n",
    "        if urls_list:\n",
    "            self.download(urls_list, threads, output_folder or key.replace(\" \", \"_\"))\n",
    "            return {\"isDownloaded\": True, \"urls_list\": urls_list, \"keyword\": key}\n",
    "        return {\"isDownloaded\": False, \"urls_list\": [], \"keyword\": key}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keyword = input(\"Enter the keyword for searching images: \")\n",
    "    output_folder = keyword.replace(\" \", \"_\")\n",
    "    scraper = PinterestImageScraper()\n",
    "    details = scraper.scrape(keyword, output_folder, {}, 10, 10)\n",
    "    if details[\"isDownloaded\"]:\n",
    "        print(\"Downloading completed !!\")\n",
    "    else:\n",
    "        print(\"Nothing to download !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f25f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
